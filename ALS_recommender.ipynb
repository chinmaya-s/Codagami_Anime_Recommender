{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import load_recommender_data\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in_str = 'data/user_datajson'\n",
    "df_merge, anime_df, user_df = load_recommender_data.load_recommender_data(path_in_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df[['anime_id', 'title', 'genres_name']]\n",
    "anime_df = anime_df.rename({'genres_name': 'genres'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.astype({'anime_id': int, 'title': str, 'genres': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = df_merge[['user_id', 'anime_id', 'rating_y']].copy()\n",
    "df_ratings = df_ratings.rename({'rating_y': 'rating'}, axis=1)\n",
    "df_ratings = df_ratings.astype({'anime_id': int, 'user_id': int, 'rating': float})\n",
    "df_ratings.reset_index(inplace=True)\n",
    "df_ratings['rating'] = ((df_ratings['rating'] / 10.0)*4.0) + 1\n",
    "df_ratings = df_ratings[['user_id', 'anime_id', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings.to_csv('data/ratings.csv', index=False)\n",
    "anime_df.to_csv('data/animes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"anime recommendation\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"96g\") \\\n",
    "    .config(\"spark.driver.memory\", \"96g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.master\", \"local[12]\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes = spark.read.load('data/animes.csv', format='csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 1.0), (2, 1, 3.8), (3, 1, 5.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_rating = sc.textFile('data/ratings.csv')\n",
    "header = anime_rating.take(1)[0]\n",
    "rating_data = anime_rating \\\n",
    "    .filter(lambda line: line!=header) \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))) \\\n",
    "    .cache()\n",
    "rating_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[546] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, validation, test = rating_data.randomSplit([6, 2, 2], seed=99)\n",
    "train.cache()\n",
    "validation.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ALS(train_data, validation_data, num_iters, reg_param, ranks):\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    for rank in ranks:\n",
    "        for reg in reg_param:\n",
    "            model = ALS.train(\n",
    "                ratings=train_data,    # (userID, productID, rating) tuple\n",
    "                iterations=num_iters,\n",
    "                rank=rank,\n",
    "                lambda_=reg,           # regularization param\n",
    "                seed=99)\n",
    "            valid_data = validation_data.map(lambda p: (p[0], p[1]))\n",
    "            predictions = model.predictAll(valid_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "            ratesAndPreds = validation_data.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "            MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "            error = math.sqrt(MSE)\n",
    "            print('{} latent factors and regularization = {}: validation RMSE is {}'.format(rank, reg, error))\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors and regularization = {}'.format(best_rank, best_regularization))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 latent factors and regularization = 0.05: validation RMSE is 1.583840405551869\n",
      "\n",
      "The best model has 8 latent factors and regularization = 0.05\n",
      "Total Runtime: 10.77 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "num_iterations = 10\n",
    "# ranks = [8, 10, 12, 14, 16, 18, 20]\n",
    "ranks = [8]\n",
    "# reg_params = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "reg_params = [0.05]\n",
    "start_time = time.time()\n",
    "final_model = train_ALS(train, validation, num_iterations, reg_params, ranks)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The out-of-sample RMSE of rating predictions is 1.5753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "test_data = test.map(lambda p: (p[0], p[1]))\n",
    "predictions = final_model.predictAll(test_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = test.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "error = math.sqrt(MSE)\n",
    "print('The out-of-sample RMSE of rating predictions is', round(error, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anime_id(df_animes, fav_anime_list):\n",
    "    anime_id_list = []\n",
    "    for anime in fav_anime_list:\n",
    "        anime_ids = df_animes \\\n",
    "            .filter(animes.title.like('%{}%'.format(anime))) \\\n",
    "            .select('anime_id') \\\n",
    "            .rdd \\\n",
    "            .map(lambda r: r[0]) \\\n",
    "            .collect()\n",
    "        anime_id_list.extend(anime_ids)\n",
    "    return list(set(anime_id_list))\n",
    "\n",
    "\n",
    "def add_new_user_to_data(train_data, anime_id_list, spark_context):\n",
    "    new_id = train_data.map(lambda r: r[0]).max() + 1\n",
    "    max_rating = train_data.map(lambda r: r[2]).max()\n",
    "    user_rows = [(new_id, anime_id, max_rating) for anime_id in anime_id_list]\n",
    "    new_rdd = spark_context.parallelize(user_rows)\n",
    "    return train_data.union(new_rdd)\n",
    "\n",
    "\n",
    "def get_inference_data(train_data, df_animes, anime_id_list):\n",
    "    new_id = train_data.map(lambda r: r[0]).max() + 1\n",
    "    return df_animes.rdd \\\n",
    "        .map(lambda r: r[0]) \\\n",
    "        .distinct() \\\n",
    "        .filter(lambda x: x not in anime_id_list) \\\n",
    "        .map(lambda x: (new_id, x))\n",
    "\n",
    "\n",
    "def make_recommendation(best_model_params, ratings_data, df_animes, \n",
    "                        fav_anime_list, n_recommendations, spark_context):\n",
    "\n",
    "    anime_id_list = get_anime_id(df_animes, fav_anime_list)\n",
    "    train_data = add_new_user_to_data(ratings_data, anime_id_list, spark_context)\n",
    "    \n",
    "    model = ALS.train(\n",
    "        ratings=train_data,\n",
    "        iterations=best_model_params.get('iterations', None),\n",
    "        rank=best_model_params.get('rank', None),\n",
    "        lambda_=best_model_params.get('lambda_', None),\n",
    "        seed=99)\n",
    "    \n",
    "    inference_rdd = get_inference_data(ratings_data, df_animes, anime_id_list)\n",
    "    \n",
    "    predictions = model.predictAll(inference_rdd).map(lambda r: (r[1], r[2]))\n",
    "    \n",
    "    topn_rows = predictions.sortBy(lambda r: r[1], ascending=False).take(n_recommendations)\n",
    "    topn_ids = [r[0] for r in topn_rows]\n",
    "    \n",
    "    return df_animes.filter(animes.anime_id.isin(topn_ids)) \\\n",
    "                    .select('title') \\\n",
    "                    .rdd \\\n",
    "                    .map(lambda r: r[0]) \\\n",
    "                    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make recommendations\n",
    "Enter your anime name in my_anime_list, and the recommender will generate recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for Cowboy Bebop:\n",
      "1: Fullmetal Alchemist: Brotherhood\n",
      "2: Hunter x Hunter (2011)\n",
      "3: Shingeki no Kyojin Season 3\n",
      "4: Shokugeki no Souma: San no Sara\n",
      "5: \"Violet Evergarden: Kitto \"\"Ai\"\" wo Shiru Hi ga Kuru no Darou\"\n",
      "6: Seishun Buta Yarou wa Yumemiru Shoujo no Yume wo Minai\n",
      "7: Death Note\n",
      "8: Kimi no Na wa.\n",
      "9: Saiki Kusuo no Î¨-nan 2\n",
      "10: Shingeki no Kyojin Season 3 Part 2\n"
     ]
    }
   ],
   "source": [
    "my_anime_list = ['Cowboy Bebop']\n",
    "\n",
    "recommends = make_recommendation(\n",
    "    best_model_params={'iterations': 10, 'rank': 20, 'lambda_': 0.05}, \n",
    "    ratings_data=rating_data, \n",
    "    df_animes=animes, \n",
    "    fav_anime_list=my_anime_list, \n",
    "    n_recommendations=10, \n",
    "    spark_context=sc)\n",
    "\n",
    "print('Recommendations for {}:'.format(my_anime_list[0]))\n",
    "for i, title in enumerate(recommends):\n",
    "    print('{0}: {1}'.format(i+1, title))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
