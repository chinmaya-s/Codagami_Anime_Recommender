{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this project, I will use an Alternating Least Squares (ALS) algorithm with Spark APIs to predict the ratings for the animes in [AnimeLens Datasets](https://grouplens.org/datasets/animelens/latest/)\n",
    "\n",
    "## [Recommender system](https://en.wikipedia.org/wiki/Recommender_system)\n",
    "A recommendation system is basically an information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. It is widely used in different internet / online business such as Amazon, Netflix, Spotify, or social media like Facebook and Youtube. By using recommender systems, those companies are able to provide better or more suited products/services/contents that are personalized to a user based on his/her historical consumer behaviors\n",
    "\n",
    "Recommender systems typically produce a list of recommendations through collaborative filtering or through content-based filtering\n",
    "\n",
    "This project will focus on collaborative filtering and use Alternating Least Squares (ALS) algorithm to make anime predictions\n",
    "\n",
    "\n",
    "## [Alternating Least Squares](https://endymecy.gitbooks.io/spark-ml-source-analysis/content/%E6%8E%A8%E8%8D%90/papers/Large-scale%20Parallel%20Collaborative%20Filtering%20the%20Netflix%20Prize.pdf)\n",
    "ALS is one of the low rank matrix approximation algorithms for collaborative filtering. ALS decomposes user-item matrix into two low rank matrixes: user matrix and item matrix. In collaborative filtering, users and products are described by a small set of latent factors that can be used to predict missing entries. And ALS algorithm learns these latent factors by matrix factorization\n",
    "\n",
    "\n",
    "## Data Sets\n",
    "I use [AnimeLens Datasets](https://grouplens.org/datasets/animelens/latest/).\n",
    "This dataset (ml-latest.zip) describes 5-star rating and free-text tagging activity from [AnimeLens](http://animelens.org), a anime recommendation service. It contains 27753444 ratings and 1108997 tag applications across 58098 animes. These data were created by 283228 users between January 09, 1995 and September 26, 2018. This dataset was generated on September 26, 2018.\n",
    "\n",
    "Users were selected at random for inclusion. All selected users had rated at least 1 animes. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "\n",
    "The data are contained in the files `genome-scores.csv`, `genome-tags.csv`, `links.csv`, `animes.csv`, `ratings.csv` and `tags.csv`.\n",
    "\n",
    "## Project Content\n",
    "1. Load Data\n",
    "2. Spark SQL and OLAP\n",
    "3. Spark ALS based approach for training model\n",
    "4. ALS Model Selection and Evaluation\n",
    "5. Model testing\n",
    "6. Make anime recommendation to myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import load_recommender_data\n",
    "\n",
    "# spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# data science imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rythm/Courses/CS685/project/Codagami_Anime_Recommender/load_recommender_data.py:27: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  user_df = melt_series(df.data).to_frame().join(df.drop('data', 1))\n",
      "/home/rythm/Courses/CS685/project/Codagami_Anime_Recommender/env/lib/python3.8/site-packages/pandas/core/frame.py:5039: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n",
      "/home/rythm/Courses/CS685/project/Codagami_Anime_Recommender/load_recommender_data.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_merge['rating_x'] = df_merge['rating_x'].astype(int)\n",
      "/home/rythm/Courses/CS685/project/Codagami_Anime_Recommender/load_recommender_data.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_merge['rating_x'] = df_merge['rating_x'].round()\n"
     ]
    }
   ],
   "source": [
    "path_in_str = 'user_data1.0.json'\n",
    "df_merge, anime_df, user_df = load_recommender_data.load_recommender_data(path_in_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df[['anime_id', 'title', 'genres_name']]\n",
    "anime_df = anime_df.rename({'genres_name': 'genres'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.astype({'anime_id': int, 'title': str, 'genres': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = df_merge[['user_id', 'anime_id', 'rating_y']].copy()\n",
    "df_ratings = df_ratings.rename({'rating_y': 'rating'}, axis=1)\n",
    "df_ratings = df_ratings.astype({'anime_id': int, 'user_id': int, 'rating': float})\n",
    "df_ratings.reset_index(inplace=True)\n",
    "df_ratings['rating'] = ((df_ratings['rating'] / 10.0)*4.0) + 1\n",
    "df_ratings = df_ratings[['user_id', 'anime_id', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49725</th>\n",
       "      <td>76</td>\n",
       "      <td>49738</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49726</th>\n",
       "      <td>77</td>\n",
       "      <td>49738</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49727</th>\n",
       "      <td>82</td>\n",
       "      <td>49738</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49728</th>\n",
       "      <td>86</td>\n",
       "      <td>49738</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49729</th>\n",
       "      <td>89</td>\n",
       "      <td>49738</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49730 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  anime_id  rating\n",
       "0            1         1     1.0\n",
       "1            2         1     3.8\n",
       "2            3         1     5.0\n",
       "3            5         1     4.2\n",
       "4            6         1     5.0\n",
       "...        ...       ...     ...\n",
       "49725       76     49738     1.0\n",
       "49726       77     49738     1.0\n",
       "49727       82     49738     1.0\n",
       "49728       86     49738     1.0\n",
       "49729       89     49738     3.4\n",
       "\n",
       "[49730 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings.to_csv('ratings.csv', index=False)\n",
    "anime_df.to_csv('animes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark config\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"anime recommendation\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"96g\") \\\n",
    "    .config(\"spark.driver.memory\", \"96g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.master\", \"local[12]\") \\\n",
    "    .getOrCreate()\n",
    "# get spark context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes = spark.read.load('animes.csv', format='csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ALS based approach for training model\n",
    "1. Reload data\n",
    "2. Split data into train, validation, test\n",
    "3. ALS model selection and evaluation\n",
    "4. Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload data\n",
    "We will use an RDD-based API from pyspark.mllib to predict the ratings, so let's reload \"ratings.csv\" using sc.textFile and then convert it to the form of (user, item, rating) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/19 17:25:38 WARN BlockManager: Task 398 already completed, not releasing lock for rdd_283_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 1, 1.0), (2, 1, 3.8), (3, 1, 5.0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "anime_rating = sc.textFile('ratings.csv')\n",
    "# preprocess data -- only need [\"userId\", \"anime_id\", \"rating\"]\n",
    "header = anime_rating.take(1)[0]\n",
    "rating_data = anime_rating \\\n",
    "    .filter(lambda line: line!=header) \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))) \\\n",
    "    .cache()\n",
    "# check three rows\n",
    "rating_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "Now we split the data into training/validation/testing sets using a 6/2/2 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[287] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, validation, test = rating_data.randomSplit([6, 2, 2], seed=99)\n",
    "# cache data\n",
    "train.cache()\n",
    "validation.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS model selection and evaluation\n",
    "With the ALS model, we can use a grid search to find the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ALS(train_data, validation_data, num_iters, reg_param, ranks):\n",
    "    \"\"\"\n",
    "    Grid Search Function to select the best model based on RMSE of hold-out data\n",
    "    \"\"\"\n",
    "    # initial\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    for rank in ranks:\n",
    "        for reg in reg_param:\n",
    "            # train ALS model\n",
    "            model = ALS.train(\n",
    "                ratings=train_data,    # (userID, productID, rating) tuple\n",
    "                iterations=num_iters,\n",
    "                rank=rank,\n",
    "                lambda_=reg,           # regularization param\n",
    "                seed=99)\n",
    "            # make prediction\n",
    "            valid_data = validation_data.map(lambda p: (p[0], p[1]))\n",
    "            predictions = model.predictAll(valid_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "            # get the rating result\n",
    "            ratesAndPreds = validation_data.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "            # get the RMSE\n",
    "            MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "            error = math.sqrt(MSE)\n",
    "            print('{} latent factors and regularization = {}: validation RMSE is {}'.format(rank, reg, error))\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors and regularization = {}'.format(best_rank, best_regularization))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 latent factors and regularization = 0.05: validation RMSE is 1.5838404042284298\n",
      "\n",
      "The best model has 8 latent factors and regularization = 0.05\n",
      "Total Runtime: 4.78 seconds\n"
     ]
    }
   ],
   "source": [
    "# hyper-param config\n",
    "num_iterations = 10\n",
    "# ranks = [8, 10, 12, 14, 16, 18, 20]\n",
    "ranks = [8]\n",
    "# reg_params = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "reg_params = [0.05]\n",
    "\n",
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model = train_ALS(train, validation, num_iterations, reg_params, ranks)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS model learning curve\n",
    "As we increase number of iterations in training ALS, we can see how RMSE changes and whether or not model is overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(arr_iters, train_data, validation_data, reg, rank):\n",
    "    \"\"\"\n",
    "    Plot function to show learning curve of ALS\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for num_iters in arr_iters:\n",
    "        # train ALS model\n",
    "        model = ALS.train(\n",
    "            ratings=train_data,    # (userID, productID, rating) tuple\n",
    "            iterations=num_iters,\n",
    "            rank=rank,\n",
    "            lambda_=reg,           # regularization param\n",
    "            seed=99)\n",
    "        # make prediction\n",
    "        valid_data = validation_data.map(lambda p: (p[0], p[1]))\n",
    "        predictions = model.predictAll(valid_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "        # get the rating result\n",
    "        ratesAndPreds = validation_data.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "        # get the RMSE\n",
    "        MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "        error = math.sqrt(MSE)\n",
    "        # add to errors\n",
    "        errors.append(error)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(arr_iters, errors)\n",
    "    plt.xlabel('number of iterations')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('ALS Learning Curve')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create an array of num_iters\n",
    "# iter_array = list(range(1, 11))\n",
    "# # create learning curve plot\n",
    "# plot_learning_curve(iter_array, train, validation, 0.05, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 3 iterations, alternating gradient descend starts to converge at an error around 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing\n",
    "And finally, make a prediction and check the testing error using out-of-sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The out-of-sample RMSE of rating predictions is 1.5753\n"
     ]
    }
   ],
   "source": [
    "# make prediction using test data\n",
    "test_data = test.map(lambda p: (p[0], p[1]))\n",
    "predictions = final_model.predictAll(test_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "# get the rating result\n",
    "ratesAndPreds = test.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "# get the RMSE\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "error = math.sqrt(MSE)\n",
    "print('The out-of-sample RMSE of rating predictions is', round(error, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make anime recommendation to myself\n",
    "We need to define a function that takes new user's anime rating and output top 10 recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anime_id(df_animes, fav_anime_list):\n",
    "    \"\"\"\n",
    "    return all anime_id(s) of user's favorite animes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_animes: spark Dataframe, animes data\n",
    "    \n",
    "    fav_anime_list: list, user's list of favorite animes\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    anime_id_list: list of anime_id(s)\n",
    "    \"\"\"\n",
    "    anime_id_list = []\n",
    "    for anime in fav_anime_list:\n",
    "        anime_ids = df_animes \\\n",
    "            .filter(animes.title.like('%{}%'.format(anime))) \\\n",
    "            .select('anime_id') \\\n",
    "            .rdd \\\n",
    "            .map(lambda r: r[0]) \\\n",
    "            .collect()\n",
    "        anime_id_list.extend(anime_ids)\n",
    "    return list(set(anime_id_list))\n",
    "\n",
    "\n",
    "def add_new_user_to_data(train_data, anime_id_list, spark_context):\n",
    "    \"\"\"\n",
    "    add new rows with new user, user's anime and ratings to\n",
    "    existing train data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: spark RDD, ratings data\n",
    "    \n",
    "    anime_id_list: list, list of anime_id(s)\n",
    "\n",
    "    spark_context: Spark Context object\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    new train data with the new user's rows\n",
    "    \"\"\"\n",
    "    # get new user id\n",
    "    new_id = train_data.map(lambda r: r[0]).max() + 1\n",
    "    # get max rating\n",
    "    max_rating = train_data.map(lambda r: r[2]).max()\n",
    "    # create new user rdd\n",
    "    user_rows = [(new_id, anime_id, max_rating) for anime_id in anime_id_list]\n",
    "    new_rdd = spark_context.parallelize(user_rows)\n",
    "    # return new train data\n",
    "    return train_data.union(new_rdd)\n",
    "\n",
    "\n",
    "def get_inference_data(train_data, df_animes, anime_id_list):\n",
    "    \"\"\"\n",
    "    return a rdd with the userid and all animes (except ones in anime_id_list)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: spark RDD, ratings data\n",
    "\n",
    "    df_animes: spark Dataframe, animes data\n",
    "    \n",
    "    anime_id_list: list, list of anime_id(s)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    inference data: Spark RDD\n",
    "    \"\"\"\n",
    "    # get new user id\n",
    "    new_id = train_data.map(lambda r: r[0]).max() + 1\n",
    "    # return inference rdd\n",
    "    return df_animes.rdd \\\n",
    "        .map(lambda r: r[0]) \\\n",
    "        .distinct() \\\n",
    "        .filter(lambda x: x not in anime_id_list) \\\n",
    "        .map(lambda x: (new_id, x))\n",
    "\n",
    "\n",
    "def make_recommendation(best_model_params, ratings_data, df_animes, \n",
    "                        fav_anime_list, n_recommendations, spark_context):\n",
    "    \"\"\"\n",
    "    return top n anime recommendation based on user's input list of favorite animes\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    best_model_params: dict, {'iterations': iter, 'rank': rank, 'lambda_': reg}\n",
    "\n",
    "    ratings_data: spark RDD, ratings data\n",
    "\n",
    "    df_animes: spark Dataframe, animes data\n",
    "\n",
    "    fav_anime_list: list, user's list of favorite animes\n",
    "\n",
    "    n_recommendations: int, top n recommendations\n",
    "\n",
    "    spark_context: Spark Context object\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    list of top n anime recommendations\n",
    "    \"\"\"\n",
    "    # modify train data by adding new user's rows\n",
    "    anime_id_list = get_anime_id(df_animes, fav_anime_list)\n",
    "    train_data = add_new_user_to_data(ratings_data, anime_id_list, spark_context)\n",
    "    \n",
    "    # train best ALS\n",
    "    model = ALS.train(\n",
    "        ratings=train_data,\n",
    "        iterations=best_model_params.get('iterations', None),\n",
    "        rank=best_model_params.get('rank', None),\n",
    "        lambda_=best_model_params.get('lambda_', None),\n",
    "        seed=99)\n",
    "    \n",
    "    # get inference rdd\n",
    "    inference_rdd = get_inference_data(ratings_data, df_animes, anime_id_list)\n",
    "    \n",
    "    # inference\n",
    "    predictions = model.predictAll(inference_rdd).map(lambda r: (r[1], r[2]))\n",
    "    \n",
    "    # get top n anime_id\n",
    "    topn_rows = predictions.sortBy(lambda r: r[1], ascending=False).take(n_recommendations)\n",
    "    topn_ids = [r[0] for r in topn_rows]\n",
    "    \n",
    "    # return anime titles\n",
    "    return df_animes.filter(animes.anime_id.isin(topn_ids)) \\\n",
    "                    .select('title') \\\n",
    "                    .rdd \\\n",
    "                    .map(lambda r: r[0]) \\\n",
    "                    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend I am a new user in this recommender system. I will input a handful of my all-time favorite animes into the system. And then the system should output top N anime recommendations for me to watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/19 17:25:46 WARN BlockManager: Task 795 already completed, not releasing lock for rdd_283_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for Cowboy Bebop:\n",
      "1: Fullmetal Alchemist: Brotherhood\n",
      "2: Hunter x Hunter (2011)\n",
      "3: Shingeki no Kyojin Season 3\n",
      "4: Shokugeki no Souma: San no Sara\n",
      "5: \"Violet Evergarden: Kitto \"\"Ai\"\" wo Shiru Hi ga Kuru no Darou\"\n",
      "6: Seishun Buta Yarou wa Yumemiru Shoujo no Yume wo Minai\n",
      "7: Death Note\n",
      "8: Kimi no Na wa.\n",
      "9: Saiki Kusuo no Ψ-nan 2\n",
      "10: Shingeki no Kyojin Season 3 Part 2\n"
     ]
    }
   ],
   "source": [
    "# my favorite animes\n",
    "my_favorite_animes = ['Cowboy Bebop']\n",
    "\n",
    "# get recommends\n",
    "recommends = make_recommendation(\n",
    "    best_model_params={'iterations': 10, 'rank': 20, 'lambda_': 0.05}, \n",
    "    ratings_data=rating_data, \n",
    "    df_animes=animes, \n",
    "    fav_anime_list=my_favorite_animes, \n",
    "    n_recommendations=10, \n",
    "    spark_context=sc)\n",
    "\n",
    "print('Recommendations for {}:'.format(my_favorite_animes[0]))\n",
    "for i, title in enumerate(recommends):\n",
    "    print('{0}: {1}'.format(i+1, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of anime recommendations look completely different than the list from my previous **KNN** model recommender. Not only it recommends animes outside of years between 2007 and 2009 periods, but also recommends animes that were less known. So this can offer users some elements of suprise so that users won't get bored by getting the same popular animes all the time.\n",
    "\n",
    "So this list of recommendations can be blended into the previous list of recommendations from **KNN** model recommender"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
